{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yz80rPMZKObK"
   },
   "source": [
    "# Aquire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfUX-Y7sQQV9"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XhZ6anWZ6G6Z"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxo34j6Qdscq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "if not os.path.exists('./logs'):\n",
    "    os.makedirs('./logs')\n",
    "if not os.path.exists('./datasets'):\n",
    "    os.makedirs('./datasets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zb8ujxWx2oka",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 2.25M 5s\n101150K .......... .......... .......... .......... .......... 88% 1.10M 5s\n101200K .......... .......... .......... .......... .......... 88% 15.1M 5s\n101250K .......... .......... .......... .......... .......... 88% 1.41M 5s\n101300K .......... .......... .......... .......... .......... 88% 32.9M 5s\n101350K .......... .......... .......... .......... .......... 88% 3.60M 5s\n101400K .......... .......... .......... .......... .......... 88% 6.94M 5s\n101450K .......... .......... .......... .......... .......... 88% 2.32M 5s\n101500K .......... .......... .......... .......... .......... 88% 15.4M 5s\n101550K .......... .......... .......... .......... .......... 89% 18.0M 5s\n101600K .......... .......... .......... .......... .......... 89%  785K 5s\n101650K .......... .......... .......... .......... .......... 89% 22.2M 5s\n101700K .......... .......... .......... .......... .......... 89% 24.0M 5s\n101750K .......... .......... .......... .......... .......... 89% 1.43M 5s\n101800K .......... .......... .......... .......... .......... 89% 10.2M 5s\n101850K .......... .......... .......... .......... .......... 89% 3.24M 5s\n101900K .......... .......... .......... .......... .......... 89% 2.13M 5s\n101950K .......... .......... .......... .......... .......... 89% 17.6M 5s\n102000K .......... .......... .......... .......... .......... 89% 22.5M 5s\n102050K .......... .......... .......... .......... .......... 89% 2.24M 5s\n102100K .......... .......... .......... .......... .......... 89% 1.13M 5s\n102150K .......... .......... .......... .......... .......... 89% 17.3M 5s\n102200K .......... .......... .......... .......... .......... 89% 11.7M 5s\n102250K .......... .......... .......... .......... .......... 89% 1.51M 5s\n102300K .......... .......... .......... .......... .......... 89% 12.1M 5s\n102350K .......... .......... .......... .......... .......... 89% 3.15M 5s\n102400K .......... .......... .......... .......... .......... 89% 2.12M 5s\n102450K .......... .......... .......... .......... .......... 89% 14.7M 5s\n102500K .......... .......... .......... .......... .......... 89% 27.2M 5s\n102550K .......... .......... .......... .......... .......... 89% 2.33M 5s\n102600K .......... .......... .......... .......... .......... 89% 1.11M 5s\n102650K .......... .......... .......... .......... .......... 89% 15.7M 5s\n102700K .......... .......... .......... .......... .......... 90% 1.67M 5s\n102750K .......... .......... .......... .......... .......... 90% 7.55M 5s\n102800K .......... .......... .......... .......... .......... 90% 3.61M 5s\n102850K .......... .......... .......... .......... .......... 90% 1.77M 5s\n102900K .......... .......... .......... .......... .......... 90% 24.2M 5s\n102950K .......... .......... .......... .......... .......... 90% 19.3M 5s\n103000K .......... .......... .......... .......... .......... 90% 19.3M 5s\n103050K .......... .......... .......... .......... .......... 90% 2.43M 5s\n103100K .......... .......... .......... .......... .......... 90% 1.08M 5s\n103150K .......... .......... .......... .......... .......... 90% 24.6M 5s\n103200K .......... .......... .......... .......... .......... 90% 1.42M 5s\n103250K .......... .......... .......... .......... .......... 90% 12.6M 5s\n103300K .......... .......... .......... .......... .......... 90% 3.00M 5s\n103350K .......... .......... .......... .......... .......... 90% 2.20M 5s\n103400K .......... .......... .......... .......... .......... 90% 15.7M 4s\n103450K .......... .......... .......... .......... .......... 90% 24.3M 4s\n103500K .......... .......... .......... .......... .......... 90% 25.2M 4s\n103550K .......... .......... .......... .......... .......... 90% 2.32M 4s\n103600K .......... .......... .......... .......... .......... 90% 1.08M 4s\n103650K .......... .......... .......... .......... .......... 90% 24.8M 4s\n103700K .......... .......... .......... .......... .......... 90% 1.44M 4s\n103750K .......... .......... .......... .......... .......... 90% 11.6M 4s\n103800K .......... .......... .......... .......... .......... 90% 2.86M 4s\n103850K .......... .......... .......... .......... .......... 91% 2.29M 4s\n103900K .......... .......... .......... .......... .......... 91% 18.0M 4s\n103950K .......... .......... .......... .......... .......... 91% 23.8M 4s\n104000K .......... .......... .......... .......... .......... 91% 2.23M 4s\n104050K .......... .......... .......... .......... .......... 91% 1.13M 4s\n104100K .......... .......... .......... .......... .......... 91% 17.4M 4s\n104150K .......... .......... .......... .......... .......... 91% 22.6M 4s\n104200K .......... .......... .......... .......... .......... 91% 1.46M 4s\n104250K .......... .......... .......... .......... .......... 91% 10.1M 4s\n104300K .......... .......... .......... .......... .......... 91% 2.75M 4s\n104350K .......... .......... .......... .......... .......... 91% 2.38M 4s\n104400K .......... .......... .......... .......... .......... 91% 13.0M 4s\n104450K .......... .......... .......... .......... .......... 91% 25.4M 4s\n104500K .......... .......... .......... .......... .......... 91% 2.31M 4s\n104550K .......... .......... .......... .......... .......... 91% 1.13M 4s\n104600K .......... .......... .......... .......... .......... 91% 16.5M 4s\n104650K .......... .......... .......... .......... .......... 91% 23.2M 4s\n104700K .......... .......... .......... .......... .......... 91% 1.45M 4s\n104750K .......... .......... .......... .......... .......... 91% 12.5M 4s\n104800K .......... .......... .......... .......... .......... 91% 2.72M 4s\n104850K .......... .......... .......... .......... .......... 91% 2.25M 4s\n104900K .......... .......... .......... .......... .......... 91% 16.1M 4s\n104950K .......... .......... .......... .......... .......... 92% 29.5M 4s\n105000K .......... .......... .......... .......... .......... 92% 2.31M 4s\n105050K .......... .......... .......... .......... .......... 92% 1.12M 4s\n105100K .......... .......... .......... .......... .......... 92% 17.9M 4s\n105150K .......... .......... .......... .......... .......... 92% 24.6M 4s\n105200K .......... .......... .......... .......... .......... 92% 1.41M 4s\n105250K .......... .......... .......... .......... .......... 92% 15.5M 4s\n105300K .......... .......... .......... .......... .......... 92% 2.84M 4s\n105350K .......... .......... .......... .......... .......... 92% 2.22M 4s\n105400K .......... .......... .......... .......... .......... 92% 16.1M 4s\n105450K .......... .......... .......... .......... .......... 92% 24.5M 4s\n105500K .......... .......... .......... .......... .......... 92% 16.0M 4s\n105550K .......... .......... .......... .......... .......... 92%  812K 4s\n105600K .......... .......... .......... .......... .......... 92% 17.5M 4s\n105650K .......... .......... .......... .......... .......... 92% 21.3M 4s\n105700K .......... .......... .......... .......... .......... 92% 1.48M 4s\n105750K .......... .......... .......... .......... .......... 92% 17.4M 3s\n105800K .......... .......... .......... .......... .......... 92% 2.68M 3s\n105850K .......... .......... .......... .......... .......... 92% 2.21M 3s\n105900K .......... .......... .......... .......... .......... 92% 24.9M 3s\n105950K .......... .......... .......... .......... .......... 92% 18.1M 3s\n106000K .......... .......... .......... .......... .......... 92% 2.29M 3s\n106050K .......... .......... .......... .......... .......... 92% 1.10M 3s\n106100K .......... .......... .......... .......... .......... 93% 29.7M 3s\n106150K .......... .......... .......... .......... .......... 93% 14.1M 3s\n106200K .......... .......... .......... .......... .......... 93% 1.51M 3s\n106250K .......... .......... .......... .......... .......... 93% 28.3M 3s\n106300K .......... .......... .......... .......... .......... 93% 3.89M 3s\n106350K .......... .......... .......... .......... .......... 93% 1.73M 3s\n106400K .......... .......... .......... .......... .......... 93% 20.8M 3s\n106450K .......... .......... .......... .......... .......... 93% 16.9M 3s\n106500K .......... .......... .......... .......... .......... 93% 29.1M 3s\n106550K .......... .......... .......... .......... .......... 93% 2.18M 3s\n106600K .......... .......... .......... .......... .......... 93% 1.14M 3s\n106650K .......... .......... .......... .......... .......... 93% 19.3M 3s\n106700K .......... .......... .......... .......... .......... 93% 1.45M 3s\n106750K .......... .......... .......... .......... .......... 93% 25.9M 3s\n106800K .......... .......... .......... .......... .......... 93% 3.94M 3s\n106850K .......... .......... .......... .......... .......... 93% 1.72M 3s\n106900K .......... .......... .......... .......... .......... 93% 24.9M 3s\n106950K .......... .......... .......... .......... .......... 93% 24.8M 3s\n107000K .......... .......... .......... .......... .......... 93% 22.4M 3s\n107050K .......... .......... .......... .......... .......... 93% 2.32M 3s\n107100K .......... .......... .......... .......... .......... 93% 1.10M 3s\n107150K .......... .......... .......... .......... .......... 93% 17.9M 3s\n107200K .......... .......... .......... .......... .......... 93% 1.68M 3s\n107250K .......... .......... .......... .......... .......... 94% 7.76M 3s\n107300K .......... .......... .......... .......... .......... 94% 12.8M 3s\n107350K .......... .......... .......... .......... .......... 94% 2.87M 3s\n107400K .......... .......... .......... .......... .......... 94% 2.25M 3s\n107450K .......... .......... .......... .......... .......... 94% 25.4M 3s\n107500K .......... .......... .......... .......... .......... 94% 19.1M 3s\n107550K .......... .......... .......... .......... .......... 94% 33.6M 3s\n107600K .......... .......... .......... .......... .......... 94%  778K 3s\n107650K .......... .......... .......... .......... .......... 94% 15.8M 3s\n107700K .......... .......... .......... .......... .......... 94% 28.0M 3s\n107750K .......... .......... .......... .......... .......... 94% 1.47M 3s\n107800K .......... .......... .......... .......... .......... 94% 33.3M 3s\n107850K .......... .......... .......... .......... .......... 94% 4.58M 3s\n107900K .......... .......... .......... .......... .......... 94% 1.62M 3s\n107950K .......... .......... .......... .......... .......... 94% 35.9M 3s\n108000K .......... .......... .......... .......... .......... 94% 15.0M 3s\n108050K .......... .......... .......... .......... .......... 94% 34.6M 3s\n108100K .......... .......... .......... .......... .......... 94% 2.31M 2s\n108150K .......... .......... .......... .......... .......... 94% 1.10M 2s\n108200K .......... .......... .......... .......... .......... 94% 15.0M 2s\n108250K .......... .......... .......... .......... .......... 94% 19.5M 2s\n108300K .......... .......... .......... .......... .......... 94% 1.52M 2s\n108350K .......... .......... .......... .......... .......... 94% 22.5M 2s\n108400K .......... .......... .......... .......... .......... 95% 2.56M 2s\n108450K .......... .......... .......... .......... .......... 95% 2.30M 2s\n108500K .......... .......... .......... .......... .......... 95% 24.6M 2s\n108550K .......... .......... .......... .......... .......... 95% 20.8M 2s\n108600K .......... .......... .......... .......... .......... 95% 24.8M 2s\n108650K .......... .......... .......... .......... .......... 95% 2.29M 2s\n108700K .......... .......... .......... .......... .......... 95% 1.11M 2s\n108750K .......... .......... .......... .......... .......... 95% 16.4M 2s\n108800K .......... .......... .......... .......... .......... 95% 1.52M 2s\n108850K .......... .......... .......... .......... .......... 95% 20.0M 2s\n108900K .......... .......... .......... .......... .......... 95% 13.7M 2s\n108950K .......... .......... .......... .......... .......... 95% 2.58M 2s\n109000K .......... .......... .......... .......... .......... 95% 2.48M 2s\n109050K .......... .......... .......... .......... .......... 95% 27.3M 2s\n109100K .......... .......... .......... .......... .......... 95% 18.0M 2s\n109150K .......... .......... .......... .......... .......... 95% 23.7M 2s\n109200K .......... .......... .......... .......... .......... 95%  784K 2s\n109250K .......... .......... .......... .......... .......... 95% 39.8M 2s\n109300K .......... .......... .......... .......... .......... 95% 15.4M 2s\n109350K .......... .......... .......... .......... .......... 95% 1.53M 2s\n109400K .......... .......... .......... .......... .......... 95% 20.0M 2s\n109450K .......... .......... .......... .......... .......... 95% 13.7M 2s\n109500K .......... .......... .......... .......... .......... 95% 2.51M 2s\n109550K .......... .......... .......... .......... .......... 96% 2.44M 2s\n109600K .......... .......... .......... .......... .......... 96% 16.1M 2s\n109650K .......... .......... .......... .......... .......... 96% 34.3M 2s\n109700K .......... .......... .......... .......... .......... 96% 18.7M 2s\n109750K .......... .......... .......... .......... .......... 96% 2.44M 2s\n109800K .......... .......... .......... .......... .......... 96% 1.12M 2s\n109850K .......... .......... .......... .......... .......... 96% 16.2M 2s\n109900K .......... .......... .......... .......... .......... 96% 1.53M 2s\n109950K .......... .......... .......... .......... .......... 96% 20.6M 2s\n110000K .......... .......... .......... .......... .......... 96% 4.59M 2s\n110050K .......... .......... .......... .......... .......... 96% 1.55M 2s\n110100K .......... .......... .......... .......... .......... 96% 44.7M 2s\n110150K .......... .......... .......... .......... .......... 96% 24.6M 2s\n110200K .......... .......... .......... .......... .......... 96% 18.8M 2s\n110250K .......... .......... .......... .......... .......... 96% 25.3M 2s\n110300K .......... .......... .......... .......... .......... 96% 2.31M 2s\n110350K .......... .......... .......... .......... .......... 96% 1.13M 2s\n110400K .......... .......... .......... .......... .......... 96% 13.9M 2s\n110450K .......... .......... .......... .......... .......... 96% 1.55M 1s\n110500K .......... .......... .......... .......... .......... 96% 18.7M 1s\n110550K .......... .......... .......... .......... .......... 96% 14.4M 1s\n110600K .......... .......... .......... .......... .......... 96% 2.88M 1s\n110650K .......... .......... .......... .......... .......... 96% 2.24M 1s\n110700K .......... .......... .......... .......... .......... 97% 27.1M 1s\n110750K .......... .......... .......... .......... .......... 97% 19.1M 1s\n110800K .......... .......... .......... .......... .......... 97% 16.5M 1s\n110850K .......... .......... .......... .......... .......... 97% 2.46M 1s\n110900K .......... .......... .......... .......... .......... 97% 1.11M 1s\n110950K .......... .......... .......... .......... .......... 97% 15.7M 1s\n111000K .......... .......... .......... .......... .......... 97% 24.9M 1s\n111050K .......... .......... .......... .......... .......... 97% 1.51M 1s\n111100K .......... .......... .......... .......... .......... 97% 17.6M 1s\n111150K .......... .......... .......... .......... .......... 97% 2.83M 1s\n111200K .......... .......... .......... .......... .......... 97% 2.16M 1s\n111250K .......... .......... .......... .......... .......... 97% 25.2M 1s\n111300K .......... .......... .......... .......... .......... 97% 23.7M 1s\n111350K .......... .......... .......... .......... .......... 97% 24.6M 1s\n111400K .......... .......... .......... .......... .......... 97% 2.44M 1s\n111450K .......... .......... .......... .......... .......... 97% 1.10M 1s\n111500K .......... .......... .......... .......... .......... 97% 16.7M 1s\n111550K .......... .......... .......... .......... .......... 97% 25.3M 1s\n111600K .......... .......... .......... .......... .......... 97% 1.47M 1s\n111650K .......... .......... .......... .......... .......... 97% 24.3M 1s\n111700K .......... .......... .......... .......... .......... 97% 2.70M 1s\n111750K .......... .......... .......... .......... .......... 97% 2.26M 1s\n111800K .......... .......... .......... .......... .......... 98% 19.6M 1s\n111850K .......... .......... .......... .......... .......... 98% 35.0M 1s\n111900K .......... .......... .......... .......... .......... 98% 23.2M 1s\n111950K .......... .......... .......... .......... .......... 98% 30.0M 1s\n112000K .......... .......... .......... .......... .......... 98% 2.39M 1s\n112050K .......... .......... .......... .......... .......... 98% 1.10M 1s\n112100K .......... .......... .......... .......... .......... 98% 16.4M 1s\n112150K .......... .......... .......... .......... .......... 98% 1.80M 1s\n112200K .......... .......... .......... .......... .......... 98% 6.95M 1s\n112250K .......... .......... .......... .......... .......... 98% 17.2M 1s\n112300K .......... .......... .......... .......... .......... 98% 2.73M 1s\n112350K .......... .......... .......... .......... .......... 98% 2.31M 1s\n112400K .......... .......... .......... .......... .......... 98% 14.1M 1s\n112450K .......... .......... .......... .......... .......... 98% 25.7M 1s\n112500K .......... .......... .......... .......... .......... 98% 24.5M 1s\n112550K .......... .......... .......... .......... .......... 98% 24.0M 1s\n112600K .......... .......... .......... .......... .......... 98% 2.50M 1s\n112650K .......... .......... .......... .......... .......... 98% 1.12M 1s\n112700K .......... .......... .......... .......... .......... 98% 15.4M 1s\n112750K .......... .......... .......... .......... .......... 98% 1.84M 1s\n112800K .......... .......... .......... .......... .......... 98% 5.90M 1s\n112850K .......... .......... .......... .......... .......... 98% 15.7M 1s\n112900K .......... .......... .......... .......... .......... 98% 2.87M 0s\n112950K .......... .......... .......... .......... .......... 99% 2.20M 0s\n113000K .......... .......... .......... .......... .......... 99% 24.3M 0s\n113050K .......... .......... .......... .......... .......... 99% 26.1M 0s\n113100K .......... .......... .......... .......... .......... 99% 28.3M 0s\n113150K .......... .......... .......... .......... .......... 99% 21.7M 0s\n113200K .......... .......... .......... .......... .......... 99%  815K 0s\n113250K .......... .......... .......... .......... .......... 99% 17.0M 0s\n113300K .......... .......... .......... .......... .......... 99% 23.4M 0s\n113350K .......... .......... .......... .......... .......... 99% 1.52M 0s\n113400K .......... .......... .......... .......... .......... 99% 18.4M 0s\n113450K .......... .......... .......... .......... .......... 99% 18.4M 0s\n113500K .......... .......... .......... .......... .......... 99% 2.84M 0s\n113550K .......... .......... .......... .......... .......... 99% 2.20M 0s\n113600K .......... .......... .......... .......... .......... 99% 22.8M 0s\n113650K .......... .......... .......... .......... .......... 99% 26.7M 0s\n113700K .......... .......... .......... .......... .......... 99% 24.6M 0s\n113750K .......... .......... .......... .......... .......... 99% 25.2M 0s\n113800K .......... .......... .......... .......... .......... 99% 2.55M 0s\n113850K .......... .......... .......... .......... .......... 99% 1.10M 0s\n113900K .......... .......... .......... .......... .......... 99% 16.3M 0s\n113950K .......... .......... .......... .......... .......... 99% 1.85M 0s\n114000K .......... .......... .......... .......... .......... 99% 5.31M 0s\n114050K .......... .......... .......... .......... .......... 99% 28.5M 0s\n114100K .......... .......... ........                        100% 3.48M=47s\n\n2020-08-03 02:36:18 (2.40 MB/s) - ‘./datasets/horse2zebra.zip’ saved [116867962/116867962]\n\nmkdir: cannot create directory ‘./datasets/horse2zebra/’: File exists\n./download_cyclegan_dataset.sh: line 14: unzip: command not found\n"
    }
   ],
   "source": [
    "\"\"\" Horse2zebra 데이터셋 다운로드\"\"\"\n",
    "!bash ./download_cyclegan_dataset.sh horse2zebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTHemRjFQQWC"
   },
   "source": [
    "# Model definition & Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAu_qett4H4v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufcX0A2Y4J_t"
   },
   "outputs": [],
   "source": [
    "img_size = 256 # 입력 이미지 사이즈 256x256 \n",
    "channels = 3\n",
    "ngf = 32 # G channels after first layer\n",
    "ndf = 64 # D channels after first layer\n",
    "\n",
    "epochs = 15 # 훈련 횟수, 200정도 까지 돌리면 좋으나, 시간 단축을 위해 15를 이용\n",
    "            # parameter를 바꿔 200으로 학습해서 결과를 확인할 수 있음\n",
    "batch_size = 2 # 배치 사이즈\n",
    "lambda_X = 10 # 하이퍼파라메터\n",
    "lambda_Y = 10\n",
    "lambda_identity_X = 0.5\n",
    "lambda_identity_Y = 0.5\n",
    "lr = 0.0002 # learning rate\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "mean_init = 0.0\n",
    "std_init = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "roIYTK_rvNJP",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Device is cpu.\n"
    }
   ],
   "source": [
    "# Cuda stuff\n",
    "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )\n",
    "print(\"Device is \" + str(device) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8M086leYqCAV"
   },
   "source": [
    "# CycleGAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Resblock\n",
    "\n",
    "\n",
    "![Resblock](./description/Resnet_block.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPy-hIOP43Ux",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "test1 통과\n"
    }
   ],
   "source": [
    "# ResidualBlock 설계\n",
    "# 입력: Tensor 출력: Resnet Output\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        block = [nn.ReflectionPad2d(1),\n",
    "                 nn.Conv2d(c, c, 3, 1, 0),\n",
    "                 nn.InstanceNorm2d(c),\n",
    "                 nn.ReLU(),\n",
    "                 nn.ReflectionPad2d(1),\n",
    "                 nn.Conv2d(c, c, 3, 1, 0),\n",
    "                 nn.InstanceNorm2d(c)]\n",
    "        \n",
    "        self.block = nn.Sequential(*block)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        ###YOUR CODE HERE \n",
    "        # Forward medthod에 Residual block의 아웃풋을 채우기  \n",
    "        # Note: 위의 Description을 참조\n",
    "        \"\"\"\n",
    "    \n",
    "#         return ????????????\n",
    "        return x+self.block(x)\n",
    "\n",
    "###Testing code ####\n",
    "test_tensor = torch.Tensor(1,3,64,64)\n",
    "R = ResidualBlock(3)\n",
    "assert(list(R(test_tensor).size()) == [1,3,64,64])\n",
    "print('test1 통과')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "![Generator](./description/Generator.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SijtRQW8aGv",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "테스트2 통과\n"
    }
   ],
   "source": [
    "# Generator 설계\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Encoding\n",
    "        model = []\n",
    "        model += [nn.ReflectionPad2d(4),\n",
    "                  nn.Conv2d(3, ngf, 9, 1, 0),\n",
    "                  nn.InstanceNorm2d(ngf),\n",
    "                  nn.ReLU()]\n",
    "        model += [nn.Conv2d(ngf, ngf*2, 4, 2, 1),\n",
    "                  nn.InstanceNorm2d(ngf*2),\n",
    "                  nn.ReLU()]\n",
    "        model += [nn.Conv2d(ngf*2, ngf*4, 4, 2, 1),\n",
    "                  nn.InstanceNorm2d(ngf*4),\n",
    "                  nn.ReLU()]\n",
    "        \n",
    "        # Transformation\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE \n",
    "        # 아래 물음표 친 곳의 코드를 채우시오\n",
    "        # 1. Generator에 Residual block 을 채우기\n",
    "        # 2. Decoding의 차원을 잘 맞춰서 원래 이미지 사이즈로 복원하기 ==> \n",
    "        # Note: 1. https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html에서 output shape 확인하고 채우기 \n",
    "                2. 매번 줄어든 이미지를 다시 2배씩 키우고 채널수를 2배 줄이기\n",
    "        \"\"\"\n",
    "        for i in range(6):\n",
    "            model += [ResidualBlock(ngf*4)]\n",
    "#             model += [????????????(ngf*4)]   # 채널 수를 그대로 유지하면서 반복시켜주는 Residual block\n",
    "        \n",
    "        # Decoding\n",
    "        model += [nn.ConvTranspose2d(ngf*4, ngf*2, kernel_size =3, stride =2, padding = 1,output_padding=1), # 줄여준 H * W 를 다시 반대로 늘려주는 과정\n",
    "                  nn.InstanceNorm2d(ngf*2),\n",
    "                  nn.ReLU()]\n",
    "#         model += [nn.ConvTranspose2d(ngf*4, ngf*2, kernel_size =?, stride =?, padding = ?, output_padding=?), # 줄여준 H * W 를 다시 반대로 늘려주는 과정\n",
    "#                   nn.InstanceNorm2d(ngf*2),\n",
    "#                   nn.ReLU()]\n",
    "\n",
    "        model += [nn.ConvTranspose2d(ngf*2, ngf, kernel_size =3, stride =2, padding = 1,output_padding=1),\n",
    "                  nn.InstanceNorm2d(ngf),\n",
    "                  nn.ReLU()]\n",
    "#         model += [nn.ConvTranspose2d(ngf*2, ngf, kernel_size =?, stride =?, padding = ?, output_padding=?),\n",
    "#                   nn.InstanceNorm2d(ngf),\n",
    "#                   nn.ReLU()]\n",
    "\n",
    "        model += [nn.ReflectionPad2d(4),\n",
    "                  nn.Conv2d(ngf, 3, 9, 1, 0),\n",
    "                  nn.Tanh()]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "### Testing code\n",
    "test_tensor = torch.Tensor(1,3,256,256)\n",
    "G= Generator()\n",
    "assert(list(G(test_tensor).size()) ==[1,3,256,256])\n",
    "print('테스트2 통과')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "![Discriminator](./description/Discriminator.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ScqGWgOGTCnx",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "테스트3 통과\n"
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE \n",
    "        # 아래 물음표 친 곳의 코드를 채우시오\n",
    "        # 1. Input, Output channel 의 사이즈를 위 그림에 맞게 넣기\n",
    "        # 2. Decoding의 차원을 잘 맞춰서 원래 이미지 사이즈로 복원하기 ==> \n",
    "        \"\"\"\n",
    "        \n",
    "        model = []\n",
    "        model += [nn.Conv2d(3, ndf, 4, 2, 1),   # outputchannel : ndf,khjn kernel: 4, stride:2 , padding : 1\n",
    "                  nn.LeakyReLU(0.2)]\n",
    "#         model += [nn.Conv2d(3, ??, ??, ??, ??),   # outputchannel : ndf, kernel: 4, stride:2 , padding : 1\n",
    "#                   nn.LeakyReLU(0.2)]\n",
    "        \n",
    "        in_channels = ndf\n",
    "        out_channels = ndf*2\n",
    "        \n",
    "        for i in range(2):\n",
    "            model += [nn.Conv2d(in_channels, out_channels, 4, 2, 1),     # 어떤 변수가 input channel이 되고, 어떤 변수가 output channel이 되는가?\n",
    "                      nn.InstanceNorm2d(out_channels),\n",
    "                      nn.LeakyReLU(0.2)]\n",
    "#             model += [nn.Conv2d(???????????, ??????????, 4, 2, 1),     # 어떤 변수가 input channel이 되고, 어떤 변수가 output channel이 되는가?\n",
    "#                       nn.InstanceNorm2d(out_channels),\n",
    "#                       nn.LeakyReLU(0.2)]\n",
    "            # 매 반복마다 channel 수가 두배가 되도록 하려면?\n",
    "            in_channels = in_channels*2   \n",
    "            out_channels = out_channels*2\n",
    "\n",
    "#             in_channels = ???????????????           \n",
    "#             out_channels = ???????????????\n",
    "\n",
    "        model += [nn.Conv2d(in_channels, out_channels, 4, 1, 1),\n",
    "                  nn.InstanceNorm2d(out_channels),\n",
    "                  nn.LeakyReLU(0.2)]\n",
    "        \n",
    "        model += [nn.Conv2d(out_channels, 1, 4, 1, 1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "### Testing code\n",
    "test_tensor = torch.Tensor(1,3,256,256)\n",
    "D= Discriminator()\n",
    "assert(list(D(test_tensor).size()) ==[1,1,30,30])\n",
    "print('테스트3 통과')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUHQaDB95Kip"
   },
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQZCce-PjX3Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Code\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class UnallignedDataset(Dataset):\n",
    "    def __init__(self, root, transform, phase='train'):\n",
    "        dir_A = os.path.join(root, phase + 'A')\n",
    "        dir_B = os.path.join(root, phase + 'B')\n",
    "        \n",
    "        self.A_paths = [os.path.join(dir_A, f) for f in os.listdir(dir_A)]\n",
    "        self.B_paths = [os.path.join(dir_B, f) for f in os.listdir(dir_B)]\n",
    "        self.A_size = len(self.A_paths)\n",
    "        self.B_size = len(self.B_paths)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        A_path = self.A_paths[index % self.A_size]\n",
    "        B_path = self.B_paths[random.randint(0, self.B_size - 1)]\n",
    "        \n",
    "        A_img = Image.open(A_path).convert('RGB')\n",
    "        B_img = Image.open(B_path).convert('RGB')\n",
    "\n",
    "        A = self.transform(A_img)\n",
    "        B = self.transform(B_img)\n",
    "        return A, B\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(self.A_size, self.B_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHht4nVj8n7F"
   },
   "outputs": [],
   "source": [
    "# 학습을 돕기 위한 추가 테크닉 (과제를 위해 알아야할 필요는 없음) (참고: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/75)\n",
    "\n",
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        self.images = []\n",
    "        \n",
    "    def get(self, img):\n",
    "        if len(self.images) < self.pool_size:\n",
    "            self.images.append(img)\n",
    "            return img\n",
    "        else:\n",
    "            p = random.random()\n",
    "            if p > 0.5:\n",
    "                idx = random.randint(0, self.pool_size-1)\n",
    "                tmp = self.images[idx]\n",
    "                self.images[idx] = img\n",
    "                return tmp\n",
    "            else:\n",
    "                return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_nWsNBQKhGz"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "il5CP_fDXnay",
    "tags": []
   },
   "outputs": [],
   "source": [
    "G = Generator().to(device)\n",
    "F = Generator().to(device)\n",
    "D_X = Discriminator().to(device)\n",
    "D_Y = Discriminator().to(device)\n",
    "G.weight_init(mean_init, std_init)\n",
    "F.weight_init(mean_init, std_init)\n",
    "D_X.weight_init(mean_init, std_init)\n",
    "D_Y.weight_init(mean_init, std_init)\n",
    "G.train()\n",
    "F.train()\n",
    "D_X.train()\n",
    "D_Y.train()\n",
    "shuffle=False\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(img_size), transforms.CenterCrop(img_size), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=UnallignedDataset('datasets/horse2zebra', transform), \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle = True, # dataloader 내의 data 들이 뒤섞여 있기를 바란다면?\n",
    "                                           pin_memory=True, \n",
    "                                           num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=UnallignedDataset('datasets/horse2zebra', transform, phase='test'), \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle = True, # dataloader 내의 data 들이 뒤섞여 있기를 바란다면?\n",
    "                                           pin_memory=True, \n",
    "                                           num_workers=0)\n",
    "\"\"\"\n",
    "### YOUR CODE HERE \n",
    "# 아래 물음표 친 곳의 코드를 채우시오\n",
    "# torch.utils.data.DataLoader 의 data를 불러오는 방식을 random으로 설정\n",
    "# Note: https://pytorch.org/docs/stable/data.html 참조\n",
    "\"\"\"\n",
    "\n",
    "# transform = transforms.Compose([transforms.Resize(img_size), transforms.CenterCrop(img_size), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=UnallignedDataset('datasets/horse2zebra', transform), \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            ???????????, \n",
    "#                                            pin_memory=True, \n",
    "#                                            num_workers=2)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=UnallignedDataset('datasets/horse2zebra', transform, phase='test'), \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            ???????????, \n",
    "#                                            pin_memory=True, \n",
    "#                                            num_workers=2)\n",
    "\n",
    "X_pool = ImagePool(50)\n",
    "Y_pool = ImagePool(50)\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "l1_criterion = nn.L1Loss()\n",
    "\n",
    "GF_optimizer = torch.optim.Adam(list(G.parameters()) + list(F.parameters()), lr=lr, betas=betas)\n",
    "D_X_optimizer = torch.optim.Adam(D_X.parameters(), lr=lr, betas=betas)\n",
    "D_Y_optimizer = torch.optim.Adam(D_Y.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "GF_scheduler = StepLR(GF_optimizer, 1, lr/100.0)\n",
    "D_X_scheduler = StepLR(D_X_optimizer, 1, lr/100.0)\n",
    "D_Y_scheduler = StepLR(D_Y_optimizer, 1, lr/100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fykPc1sCQQWU",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n   ReflectionPad2d-1          [-1, 3, 264, 264]               0\n            Conv2d-2         [-1, 32, 256, 256]           7,808\n    InstanceNorm2d-3         [-1, 32, 256, 256]               0\n              ReLU-4         [-1, 32, 256, 256]               0\n            Conv2d-5         [-1, 64, 128, 128]          32,832\n    InstanceNorm2d-6         [-1, 64, 128, 128]               0\n              ReLU-7         [-1, 64, 128, 128]               0\n            Conv2d-8          [-1, 128, 64, 64]         131,200\n    InstanceNorm2d-9          [-1, 128, 64, 64]               0\n             ReLU-10          [-1, 128, 64, 64]               0\n  ReflectionPad2d-11          [-1, 128, 66, 66]               0\n           Conv2d-12          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-13          [-1, 128, 64, 64]               0\n             ReLU-14          [-1, 128, 64, 64]               0\n  ReflectionPad2d-15          [-1, 128, 66, 66]               0\n           Conv2d-16          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-17          [-1, 128, 64, 64]               0\n    ResidualBlock-18          [-1, 128, 64, 64]               0\n  ReflectionPad2d-19          [-1, 128, 66, 66]               0\n           Conv2d-20          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-21          [-1, 128, 64, 64]               0\n             ReLU-22          [-1, 128, 64, 64]               0\n  ReflectionPad2d-23          [-1, 128, 66, 66]               0\n           Conv2d-24          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-25          [-1, 128, 64, 64]               0\n    ResidualBlock-26          [-1, 128, 64, 64]               0\n  ReflectionPad2d-27          [-1, 128, 66, 66]               0\n           Conv2d-28          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-29          [-1, 128, 64, 64]               0\n             ReLU-30          [-1, 128, 64, 64]               0\n  ReflectionPad2d-31          [-1, 128, 66, 66]               0\n           Conv2d-32          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-33          [-1, 128, 64, 64]               0\n    ResidualBlock-34          [-1, 128, 64, 64]               0\n  ReflectionPad2d-35          [-1, 128, 66, 66]               0\n           Conv2d-36          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-37          [-1, 128, 64, 64]               0\n             ReLU-38          [-1, 128, 64, 64]               0\n  ReflectionPad2d-39          [-1, 128, 66, 66]               0\n           Conv2d-40          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-41          [-1, 128, 64, 64]               0\n    ResidualBlock-42          [-1, 128, 64, 64]               0\n  ReflectionPad2d-43          [-1, 128, 66, 66]               0\n           Conv2d-44          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-45          [-1, 128, 64, 64]               0\n             ReLU-46          [-1, 128, 64, 64]               0\n  ReflectionPad2d-47          [-1, 128, 66, 66]               0\n           Conv2d-48          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-49          [-1, 128, 64, 64]               0\n    ResidualBlock-50          [-1, 128, 64, 64]               0\n  ReflectionPad2d-51          [-1, 128, 66, 66]               0\n           Conv2d-52          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-53          [-1, 128, 64, 64]               0\n             ReLU-54          [-1, 128, 64, 64]               0\n  ReflectionPad2d-55          [-1, 128, 66, 66]               0\n           Conv2d-56          [-1, 128, 64, 64]         147,584\n   InstanceNorm2d-57          [-1, 128, 64, 64]               0\n    ResidualBlock-58          [-1, 128, 64, 64]               0\n  ConvTranspose2d-59         [-1, 64, 128, 128]          73,792\n   InstanceNorm2d-60         [-1, 64, 128, 128]               0\n             ReLU-61         [-1, 64, 128, 128]               0\n  ConvTranspose2d-62         [-1, 32, 256, 256]          18,464\n   InstanceNorm2d-63         [-1, 32, 256, 256]               0\n             ReLU-64         [-1, 32, 256, 256]               0\n  ReflectionPad2d-65         [-1, 32, 264, 264]               0\n           Conv2d-66          [-1, 3, 256, 256]           7,779\n             Tanh-67          [-1, 3, 256, 256]               0\n================================================================\nTotal params: 2,042,883\nTrainable params: 2,042,883\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 372.66\nParams size (MB): 7.79\nEstimated Total Size (MB): 381.20\n----------------------------------------------------------------\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 128, 128]           3,136\n         LeakyReLU-2         [-1, 64, 128, 128]               0\n            Conv2d-3          [-1, 128, 64, 64]         131,200\n    InstanceNorm2d-4          [-1, 128, 64, 64]               0\n         LeakyReLU-5          [-1, 128, 64, 64]               0\n            Conv2d-6          [-1, 256, 32, 32]         524,544\n    InstanceNorm2d-7          [-1, 256, 32, 32]               0\n         LeakyReLU-8          [-1, 256, 32, 32]               0\n            Conv2d-9          [-1, 512, 31, 31]       2,097,664\n   InstanceNorm2d-10          [-1, 512, 31, 31]               0\n        LeakyReLU-11          [-1, 512, 31, 31]               0\n           Conv2d-12            [-1, 1, 30, 30]           8,193\n================================================================\nTotal params: 2,764,737\nTrainable params: 2,764,737\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 45.27\nParams size (MB): 10.55\nEstimated Total Size (MB): 56.57\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(G, (3, 256, 256))\n",
    "summary(D_X, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61scdU94hpqL",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2717908992 bytes. Buy new RAM!\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-873a0f872299>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Translate from X to Y, check D_Y output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mG_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mD_Y_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD_Y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mG_gan_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmse_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_Y_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_Y_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-95bec4bdfe71>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;31m### Testing code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mtest_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 342\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2717908992 bytes. Buy new RAM!\n"
     ]
    }
   ],
   "source": [
    "def mean(lst):\n",
    "    return sum(lst)/len(lst)\n",
    "\n",
    "# Prepare some test data, 5 of each kind\n",
    "test_data = [(x.to(device), y.to(device)) for i, (x, y) in enumerate(test_loader) if i<5]\n",
    "\n",
    "# Define target vectors\n",
    "fake_target = 0.0\n",
    "real_target = 1.0\n",
    "for epoch in range(epochs):\n",
    "    G_gan_loss_epoch = []\n",
    "    G_cycle_loss_epoch = []\n",
    "    G_ident_loss_epoch = []\n",
    "    D_X_gan_loss_epoch = []\n",
    "    \n",
    "    # Linear lr decay\n",
    "    if epoch > 99:\n",
    "        GF_scheduler.step()\n",
    "        D_X_scheduler.step()\n",
    "        D_Y_scheduler.step()\n",
    "        \n",
    "    for i, (X, Y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        #########################################################\n",
    "        # Update generators\n",
    "        #########################################################\n",
    "        GF_optimizer.zero_grad()\n",
    "        \n",
    "        # Translate from X to Y, check D_Y output\n",
    "        G_out = G(X)\n",
    "        D_Y_out = D_Y(G_out)\n",
    "        G_gan_loss = mse_criterion(D_Y_out, torch.ones_like(D_Y_out).to(device))\n",
    "        \n",
    "        # Translate from Y to X, check D_X output\n",
    "        F_out = F(Y)\n",
    "        D_X_out = D_X(F_out)\n",
    "        F_gan_loss = mse_criterion(D_X_out, torch.ones_like(D_X_out).to(device))\n",
    "        \n",
    "        # Translate from X to Y to X, check reconstruction error\n",
    "        X_recon = F(G_out)\n",
    "        G_cycle_loss = l1_criterion(X_recon, X) * lambda_X\n",
    "        \n",
    "        # Translate from Y to X to Y, check reconstruction error\n",
    "        Y_recon = G(F_out)\n",
    "        F_cycle_loss = l1_criterion(Y_recon, Y) * lambda_Y\n",
    "        \n",
    "        # Translate a picture from Y from X to Y, should be Y\n",
    "        Y_ident = G(Y)\n",
    "        G_ident_loss = l1_criterion(Y_ident, Y) * lambda_identity_X * lambda_X\n",
    "        \n",
    "        # Translate a picture from X from Y to X, should be X\n",
    "        X_ident = F(X)\n",
    "        F_ident_loss = l1_criterion(X_ident, X) * lambda_identity_X * lambda_Y\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE \n",
    "        # 아래 물음표 친 곳의 코드를 채우시오\n",
    "        # 1. Generator loss를 완성하시오\n",
    "        # 2. Discriminator의 loss를 완성하시오\n",
    "        # 3. loss와 optimizer의 update를 완성하시오\n",
    "        # Note: https://pytorch.org/docs/stable/data.html 참조\n",
    "        \"\"\"\n",
    "        GF_loss = G_cycle_loss + F_gan_loss + G_ident_loss + F_cycle_loss + G_gan_loss + F_ident_loss \n",
    "\n",
    "        # GF_loss = G_cycle_loss + ?????????? + G_ident_loss + ???????????? + G_gan_loss + ??????????? \n",
    "        GF_loss.backward()\n",
    "        GF_optimizer.step()\n",
    "        \n",
    "        #########################################################\n",
    "        # Update discriminators\n",
    "        # D_Y, minimize L_D_Y = E_y (D(y) - 1) ^2 + E_x (D(x))^2\n",
    "        #########################################################\n",
    "        D_Y_optimizer.zero_grad()\n",
    "        \n",
    "        # Test D_Y with fake and real input\n",
    "        G_out = Y_pool.get(G_out)\n",
    "        D_Y_out_fake = D_Y(G_out.detach())\n",
    "        D_Y_out_real = D_Y(Y)\n",
    "        # Calculate loss\n",
    "        D_Y_loss_fake = mse_criterion(D_Y_out_fake, torch.zeros_like(D_Y_out_fake).to(device))\n",
    "        D_Y_loss_real = mse_criterion(D_Y_out_real, torch.ones_like(D_Y_out_real).to(device))\n",
    "        D_Y_gan_loss = (D_Y_loss_real + D_Y_loss_fake)*0.5\n",
    "        \n",
    "        D_Y_gan_loss.backward() # back propagation 해주기\n",
    "        D_Y_optimizer.step() # optimizer가 한 스텝 나아가기\n",
    "        \n",
    "        # D_Y_gan_loss.???????? # back propagation 해주기\n",
    "        # D_Y_optimizer.?????? # optimizer가 한 스텝 나아가기\n",
    "        \n",
    "        #########################################################\n",
    "        # D_X, minimize L_D_X = E_x (D(x) - 1) ^2 + E_y (D(y))^2\n",
    "        #########################################################\n",
    "        D_X_optimizer.zero_grad()\n",
    "        \n",
    "        # Test D_X with fake and real input\n",
    "        F_out = X_pool.get(F_out)\n",
    "        D_X_out_fake = D_X(F_out.detach())\n",
    "        D_X_out_real = D_X(X)\n",
    "        # Calculate loss\n",
    "        D_X_loss_fake = mse_criterion(D_X_out_fake, torch.zeros_like(D_X_out_fake).to(device))\n",
    "        D_X_loss_real = mse_criterion(D_X_out_real, torch.ones_like(D_X_out_real).to(device))\n",
    "        D_X_gan_loss = (D_X_loss_real + D_X_loss_fake)*0.5\n",
    "        \n",
    "        D_X_gan_loss.backward() # back propagation 해주기\n",
    "        D_X_optimizer.step() # optimizer가 한 스텝 나아가기\n",
    "\n",
    "#         D_X_gan_loss.???????? # back propagation 해주기\n",
    "#         D_X_optimizer.?????? # optimizer가 한 스텝 나아가기\n",
    "                \n",
    "        # Save losses\n",
    "        G_gan_loss_epoch.append(G_gan_loss.item())\n",
    "        G_cycle_loss_epoch.append(G_cycle_loss.item())\n",
    "        G_ident_loss_epoch.append(G_ident_loss.item())\n",
    "        D_X_gan_loss_epoch.append(D_X_gan_loss.item())\n",
    "        \n",
    "        # Do some test output every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            checkname = 'Epoch [%d/%d], Batch [%d/%d]' % (epoch+1, epochs, i, len(train_loader))\n",
    "            savename = './logs/Epoch%dBatch%d' % (epoch+1, i)\n",
    "            print(checkname)\n",
    "            \n",
    "            image_tensor = None\n",
    "            # Generate test outputs\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                G.eval()\n",
    "                F.eval()\n",
    "                for X, Y in test_data:\n",
    "                    G_out = G(X)\n",
    "                    F_out = F(Y)\n",
    "                    if image_tensor is None:\n",
    "                        image_tensor = torch.cat((X, G_out, Y, F_out), 0)\n",
    "                    else:\n",
    "                        image_tensor = torch.cat((image_tensor, X, G_out, Y, F_out), 0)\n",
    "                G.train()\n",
    "                F.train()\n",
    "            save_image(image_tensor, savename + '.png', nrow=4, padding=50)\n",
    "            \n",
    "#             save_image(image_tensor, './i.' nrow=4, padding=2, normalize=True)\n",
    "#             writer.add_image('test_images', image, i+epoch*len(train_loader))\n",
    "    \n",
    "    # Calculate mean\n",
    "    G_gan_loss_epoch = mean(G_gan_loss_epoch)\n",
    "    G_cycle_loss_epoch = mean(G_cycle_loss_epoch)\n",
    "    G_ident_loss_epoch = mean(G_ident_loss_epoch)\n",
    "    G_loss_epoch = G_gan_loss_epoch + G_cycle_loss_epoch + G_ident_loss_epoch\n",
    "    D_X_gan_loss_epoch = mean(D_X_gan_loss_epoch)\n",
    "  \n",
    "print('학습 완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQ2CQ5KnQQWX"
   },
   "outputs": [],
   "source": [
    "# 학습된 parameter 저장하기\n",
    "torch.save(G.state_dict(), 'G.pt')\n",
    "torch.save(F.state_dict(), 'F.pt')\n",
    "torch.save(D_X.state_dict(), 'D_X.pt')\n",
    "torch.save(D_Y.state_dict(), 'D_Y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cyclegan_horse2zebra_assign.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}