{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL-PPO-Problem(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import gym\n",
    "import numpy as np\n",
    "env_name = 'MountainCarContinuous-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'CartPole-v1' 환경에서 랜덤 액션을 취했을때, 가지게 되는 경험의 시퀀스 (state, action, reward)를 저장하고, 이 랜던 행동을 취했을 때 얻게된  accumulated_reward의 평균값을 구하시오.\n",
    "\n",
    "- env.reset()을 하면 initial state을 반환합니다.\n",
    "- env.action_space.sample()을 하면 랜덤 action이 반환됩니다.\n",
    "- env.step(action)은 state, reward, done, info 를 반환합니다.\n",
    "\n",
    "아래의 예시를 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43637613,  0.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5887467], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P2.1 한 에피소드 동안 가지게 되는 state, action, reward를 각각의 리스트(states, actions, rewards)에 저장하세요.(아래의 코드에서 작성하시면 됩니다.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    states.append(state)\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P2.2 위에서 구한 rewards를 사용해서 한 에피소드동안 받은 reward의 총 합인 accumulated_reward를 구하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_reward = np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P2.3 10개의 에피소드에 대해 위 과정을 반복하여, accumulated_reward의 평균값을 구하세요.\n",
    "각 에피소드동안 받은 accumulated_reward를 구하고, 10개에 대한 accumulated_reward의 평균값을 구하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_rewards = []\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    accumulated_reward = 0\n",
    "    for _ in range(1000):\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        accumulated_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    accumulated_rewards.append(accumulated_reward)\n",
    "\n",
    "expected_accumulated_reward = np.mean(accumulated_reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P2.4 PPO의 Clipped Objective를 완성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_t[  min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t) ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_venv",
   "language": "python",
   "name": "base_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}